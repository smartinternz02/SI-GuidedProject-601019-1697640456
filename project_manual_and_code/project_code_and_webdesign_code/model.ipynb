{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "##Importing the Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "train_data = pd.read_csv('/content/train_u6lujuX_CVtuZ9i.csv')\n",
        "test_data = pd.read_csv('/content/test_Y3wMUE5_7gLdaTN.csv')\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "##Data Visualization and Analysis\n",
        "###Train Dataset\n",
        "train_df\n",
        "train_df.info()\n",
        "train_df.describe()\n",
        "###Univariate Analysis\n",
        "# 2. Frequency Distribution for Categorical Variables\n",
        "categorical_vars = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area', 'Loan_Status']\n",
        "frequency_distributions = {var: train_df[var].value_counts() for var in categorical_vars}\n",
        "####Bar Charts\n",
        "# Create a frequency table for the 'Gender' variable\n",
        "gender_counts = train_df['Gender'].value_counts()\n",
        "\n",
        "# Create a bar chart of the gender counts\n",
        "plt.bar(gender_counts.index, gender_counts.values)\n",
        "\n",
        "# Set the chart title and labels\n",
        "plt.title('Gender Distribution of Loan Borrowers')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Number of Borrowers')\n",
        "\n",
        "# Show the chart\n",
        "plt.show()\n",
        "####Histograms\n",
        "numerical_vars = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n",
        "for var in numerical_vars:\n",
        "    plt.hist(train_df[var], bins=20)\n",
        "    plt.title(f'Histogram of {var}')\n",
        "    plt.xlabel(var)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "####Box Plots\n",
        "for var in numerical_vars:\n",
        "    sns.boxplot(x=train_df[var])\n",
        "    plt.title(f'Box Plot of {var}')\n",
        "    plt.show()\n",
        "####Pi-chart\n",
        "for var in ['Gender', 'Education']:\n",
        "    train_df[var].value_counts().plot.pie(autopct='%1.1f%%')\n",
        "    plt.title(f'Pie Chart of {var}')\n",
        "    plt.ylabel('')\n",
        "    plt.show()\n",
        "####Kernel Density Estimation (KDE)\n",
        "for var in numerical_vars:\n",
        "    sns.kdeplot(train_df[var], shade=True)\n",
        "    plt.title(f'KDE Plot of {var}')\n",
        "    plt.xlabel(var)\n",
        "    plt.ylabel('Density')\n",
        "    plt.show()\n",
        "###Bivariate Analysis\n",
        "####Correlation Matrix\n",
        "correlation_matrix = train_df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "####Scatter Plot\n",
        "plt.scatter(train_df['ApplicantIncome'], train_df['LoanAmount'])\n",
        "plt.title('Scatter Plot: ApplicantIncome vs. LoanAmount')\n",
        "plt.xlabel('ApplicantIncome')\n",
        "plt.ylabel('LoanAmount')\n",
        "plt.show()\n",
        "####Stacked Bar Plots\n",
        "crosstab = pd.crosstab(train_df['Credit_History'], train_df['Loan_Status'])\n",
        "crosstab.plot(kind='bar', stacked=True)\n",
        "plt.title('Stacked Bar Plot: Credit History vs. Loan Status')\n",
        "plt.xlabel('Credit History')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "####Pair Plots\n",
        "sns.pairplot(train_df, hue='Loan_Status', diag_kind='kde')\n",
        "plt.suptitle('Pairplot of Numerical Variables with Loan Status')\n",
        "plt.show()\n",
        "####Violin Plots\n",
        "sns.violinplot(x='Loan_Status', y='ApplicantIncome', data=train_df)\n",
        "plt.title('Violin Plot: Loan Status vs. Applicant Income')\n",
        "plt.xlabel('Loan Status')\n",
        "plt.ylabel('Applicant Income')\n",
        "plt.show()\n",
        "####Heat map for cross tabulation\n",
        "crosstab = pd.crosstab(train_df['Education'], train_df['Loan_Status'])\n",
        "sns.heatmap(crosstab, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title('Heatmap: Education vs. Loan Status')\n",
        "plt.show()\n",
        "####Regression Plot\n",
        "sns.regplot(x='ApplicantIncome', y='LoanAmount', data=train_df)\n",
        "plt.title('Regression Plot: Applicant Income vs. Loan Amount')\n",
        "plt.xlabel('Applicant Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.show()\n",
        "###Multivariate Analysis\n",
        "train_df.plot.line()\n",
        "##Data pre-processing\n",
        "###Check for NULL values and filling them\n",
        "train_df.isnull().sum()\n",
        "train_df['LoanAmount']=train_df['LoanAmount'].fillna(train_df['LoanAmount'].mean())\n",
        "train_df['Loan_Amount_Term']=train_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].mean())\n",
        "train_df['Credit_History']=train_df['Credit_History'].fillna(train_df['Credit_History'].mean())\n",
        "train_df['Gender']=train_df['Gender'].fillna(train_df['Gender'].mode()[0])\n",
        "train_df['Married']=train_df['Married'].fillna(train_df['Married'].mode()[0])\n",
        "train_df['Dependents']=train_df['Dependents'].fillna(train_df['Dependents'].mode()[0])\n",
        "train_df['Self_Employed']=train_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0])\n",
        "###Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['Gender'] = label_encoder.fit_transform(train_df['Gender'])\n",
        "train_df['Married'] = label_encoder.fit_transform(train_df['Married'])\n",
        "train_df['Dependents'] = label_encoder.fit_transform(train_df['Dependents'])\n",
        "train_df['Education'] = label_encoder.fit_transform(train_df['Education'])\n",
        "train_df['Self_Employed'] = label_encoder.fit_transform(train_df['Self_Employed'])\n",
        "train_df['Property_Area'] = label_encoder.fit_transform(train_df['Property_Area'])\n",
        "train_df['Loan_Status'] = label_encoder.fit_transform(train_df['Loan_Status'])\n",
        "###Feature Scaling (Standardization)\n",
        "train_df=train_df.drop(columns=[\"Loan_ID\"],axis=1)\n",
        "x=train_df.iloc[:,:-1]\n",
        "y=train_df.Loan_Status\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "x_scaled_df = pd.DataFrame(x_scaled, columns=x.columns)\n",
        "x_scaled_df.head()\n",
        "y\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Assuming you have X and y as your feature matrix and target variable\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(x_scaled, y)\n",
        "X_resampled.shape,x.shape, y.shape, y_resampled.shape\n",
        "ax=y_resampled.value_counts().plot.pie(autopct='%.2f')\n",
        "_=ax.set_title(\"under-sampling\")\n",
        "###Splitting the Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X_resampled,y_resampled,test_size=0.3,random_state=10)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
        "##Model Building\n",
        "###Decision Tree Model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_1 = DecisionTreeClassifier(random_state=42)\n",
        "model_1.fit(x_train, y_train)\n",
        "model_1_predictions = model_1.predict(x_test)\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"Decision Tree Model:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model_1_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model_1_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, model_1_predictions))\n",
        "####Hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for Decision Tree\n",
        "param_grid_dt = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=5, scoring='accuracy')\n",
        "grid_search_dt.fit(x_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_dt = grid_search_dt.best_params_\n",
        "\n",
        "# Train the Decision Tree model with the best parameters\n",
        "final_dt_model = DecisionTreeClassifier(**best_params_dt, random_state=42)\n",
        "final_dt_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "dt_predictions = final_dt_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Model:\")\n",
        "print(\"Best Parameters:\", best_params_dt)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, dt_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, dt_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, dt_predictions))\n",
        "\n",
        "####Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform Cross-Validation\n",
        "cv_scores_dt = cross_val_score(final_dt_model, x_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Model:\")\n",
        "print(\"Best Parameters:\", best_params_dt)\n",
        "print(\"Cross-Validation Scores:\", cv_scores_dt)\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_scores_dt))\n",
        "\n",
        "###Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_2 = RandomForestClassifier(random_state=42)\n",
        "model_2.fit(x_train, y_train)\n",
        "model_2_predictions = model_2.predict(x_test)\n",
        "\n",
        "# Calculate accuracy as a performance metric\n",
        "print(\"Random Forest classifier:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model_2_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model_2_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, model_2_predictions))\n",
        "####Hyper-parameter tuning\n",
        "# Define the parameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')\n",
        "grid_search_rf.fit(x_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "\n",
        "# Train the Random Forest model with the best parameters\n",
        "final_rf_model = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "final_rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "rf_predictions = final_rf_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Model:\")\n",
        "print(\"Best Parameters:\", best_params_rf)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, rf_predictions))\n",
        "####Validation\n",
        "# Perform Cross-Validation\n",
        "cv_scores_rf = cross_val_score(final_rf_model, x_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Model:\")\n",
        "print(\"Best Parameters:\", best_params_rf)\n",
        "print(\"Cross-Validation Scores:\", cv_scores_rf)\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_scores_rf))\n",
        "\n",
        "###KNN Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model_3 = KNeighborsClassifier()\n",
        "model_3.fit(x_train, y_train)\n",
        "model_3_predictions = model_3.predict(x_test)\n",
        "\n",
        "# Calculate accuracy as a performance metric\n",
        "print(\"KNN:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model_3_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model_3_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, model_3_predictions))\n",
        "\n",
        "####Hyper-parameter tuning\n",
        "# Define the parameter grid for KNN\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "}\n",
        "\n",
        "# Create the KNN model\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, cv=5, scoring='accuracy')\n",
        "grid_search_knn.fit(x_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_knn = grid_search_knn.best_params_\n",
        "\n",
        "# Train the KNN model with the best parameters\n",
        "final_knn_model = KNeighborsClassifier(**best_params_knn)\n",
        "final_knn_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "knn_predictions = final_knn_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"KNN Model:\")\n",
        "print(\"Best Parameters:\", best_params_knn)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, knn_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, knn_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, knn_predictions))\n",
        "####Validation\n",
        "# Perform Cross-Validation\n",
        "cv_scores_knn = cross_val_score(final_knn_model, x_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"KNN Model:\")\n",
        "print(\"Best Parameters:\", best_params_knn)\n",
        "print(\"Cross-Validation Scores:\", cv_scores_knn)\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_scores_knn))\n",
        "\n",
        "###XG Boost classifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model_4 = XGBClassifier(random_state=42)\n",
        "model_4.fit(x_train, y_train)\n",
        "model_4_predictions = model_4.predict(x_test)\n",
        "\n",
        "# Calculate accuracy as a performance metric\n",
        "print(\"XG Boost:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model_4_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model_4_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, model_4_predictions))\n",
        "\n",
        "####Hyper-parameter tuning\n",
        "# Define the parameter grid for XGBoost\n",
        "param_grid_xgb = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Create the XGBoost model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='accuracy')\n",
        "grid_search_xgb.fit(x_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_xgb = grid_search_xgb.best_params_\n",
        "\n",
        "# Train the XGBoost model with the best parameters\n",
        "final_xgb_model = XGBClassifier(**best_params_xgb, random_state=42)\n",
        "final_xgb_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "xgb_predictions = final_xgb_model.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"XGBoost Model:\")\n",
        "print(\"Best Parameters:\", best_params_xgb)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, xgb_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, xgb_predictions))\n",
        "####Validation\n",
        "cv_scores_xgb = cross_val_score(final_xgb_model, x_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"XGBoost Model:\")\n",
        "print(\"Best Parameters:\", best_params_xgb)\n",
        "print(\"Cross-Validation Scores:\", cv_scores_xgb)\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_scores_xgb))\n",
        "\n",
        "###Ensemble Model\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Create the ensemble model using a VotingClassifier\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('DecisionTree', model_1),\n",
        "        ('RandomForest', model_2),\n",
        "        ('KNN', model_3),\n",
        "        ('XGBoost', model_4)\n",
        "    ],\n",
        "    voting='hard'  # 'hard' for majority class voting\n",
        ")\n",
        "\n",
        "# Train the ensemble model on the undersampled training data\n",
        "ensemble_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "ensemble_predictions = ensemble_model.predict(x_test)\n",
        "\n",
        "# Calculate accuracy as a performance metric for the ensemble model\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
        "print(\"Ensemble Model Accuracy:\", ensemble_accuracy)\n",
        "\n",
        "print(\"Ensemble Model:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, ensemble_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ensemble_predictions))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, ensemble_predictions))\n",
        "import joblib\n",
        "\n",
        "# Save the best model to a file\n",
        "joblib.dump(ensemble_model, 'best_model.pkl')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
